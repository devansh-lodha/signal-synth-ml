{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1e23b4de",
   "metadata": {},
   "source": [
    "# Task 1: Visualizing Gradient Descent Algorithms\n",
    "\n",
    "This notebook explores and visualizes the behavior of several fundamental optimization algorithms used in machine learning. We will compare:\n",
    "\n",
    "1.  **Vanilla Gradient Descent Variants:**\n",
    "    *   Batch Gradient Descent\n",
    "    *   Stochastic Gradient Descent (SGD)\n",
    "    *   Mini-Batch Gradient Descent\n",
    "2.  **Momentum-Based Variants:**\n",
    "    *   Batch GD with Momentum\n",
    "    *   SGD with Momentum\n",
    "    *   Mini-Batch GD with Momentum\n",
    "\n",
    "Our goal is to fit a simple linear regression model and observe how each algorithm navigates the loss landscape towards the optimal solution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f96be1a",
   "metadata": {},
   "source": [
    "### The Math: Gradient Descent Update Rule\n",
    "\n",
    "The core idea of gradient descent is to iteratively update the model parameters ($\\theta$) in the opposite direction of the gradient of the loss function ($J(\\theta)$).\n",
    "\n",
    "**Standard Update Rule:**\n",
    "$$ \\theta_{t+1} = \\theta_t - \\eta \\nabla J(\\theta_t) $$\n",
    "where $\\eta$ is the learning rate.\n",
    "\n",
    "**Update Rule with Momentum:**\n",
    "Momentum introduces a velocity term ($v_t$) that accumulates gradients over time, helping to accelerate convergence and overcome local minima.\n",
    "$$ v_{t+1} = \\gamma v_t + \\nabla J(\\theta_t) $$\n",
    "$$ \\theta_{t+1} = \\theta_t - \\eta v_{t+1} $$\n",
    "where $\\gamma$ is the momentum coefficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e4f63ffc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/devanshlodha/Documents/github/signal-synth-ml/signal-synth-env/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Add the src directory to the Python path\n",
    "# This allows us to import our custom modules\n",
    "sys.path.append(os.path.abspath(os.path.join('..', 'src')))\n",
    "\n",
    "from training import train_linear_regression_gd\n",
    "from visualize import generate_gradient_descent_gif\n",
    "\n",
    "# Setup device for training\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1434fb46",
   "metadata": {},
   "source": [
    "## 1. Data Generation\n",
    "\n",
    "We create a simple synthetic dataset based on a linear relationship with some added Gaussian noise. Our model will try to recover the original line `y = 3x + 4`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1fa9385d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set a seed for reproducibility\n",
    "np.random.seed(45)\n",
    "num_samples = 100\n",
    "\n",
    "# Generate data: y = 3x + 4 + noise\n",
    "x1_np = np.random.uniform(-2, 2, num_samples)\n",
    "f_x = 3 * x1_np + 4\n",
    "eps = np.random.randn(num_samples) * 1.5 # Add some noise\n",
    "y_np = f_x + eps\n",
    "\n",
    "# Convert to PyTorch tensors and move to the selected device\n",
    "x = torch.tensor(x1_np, dtype=torch.float32, device=device)\n",
    "y = torch.tensor(y_np, dtype=torch.float32, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a25132e",
   "metadata": {},
   "source": [
    "## 2. Training and Visualization\n",
    "\n",
    "We'll now run each optimization algorithm and generate a GIF to visualize its path on the loss contour plot. First, we need to compute the loss landscape to create the contour plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ccbd3dda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the loss landscape for visualization\n",
    "theta1_range = np.linspace(-2, 8, 100) # Range for slope (theta_1)\n",
    "theta0_range = np.linspace(-2, 8, 100) # Range for intercept (theta_0)\n",
    "SLOPE, INTERCEPT = np.meshgrid(theta1_range, theta0_range)\n",
    "\n",
    "# Calculate loss for each point in the grid\n",
    "loss_grid_np = np.zeros(SLOPE.shape)\n",
    "for i in range(SLOPE.shape[0]):\n",
    "    for j in range(SLOPE.shape[1]):\n",
    "        slope_ij = SLOPE[i, j]\n",
    "        intercept_ij = INTERCEPT[i, j]\n",
    "        # Calculate Mean Squared Error\n",
    "        loss_grid_np[i, j] = ((y.cpu().numpy() - x.cpu().numpy() * slope_ij - intercept_ij) ** 2).mean()\n",
    "\n",
    "# We'll use a log scale for better visualization of the contours\n",
    "loss_grid_for_plot = (INTERCEPT, SLOPE, np.log(loss_grid_np))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c02fd5b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Running BATCH ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training with batch: 100%|██████████| 50/50 [00:00<00:00, 243.15it/s, loss=2.1947]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final parameters: theta0=4.0512, theta1=2.9408\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating GIF frames: 100%|██████████| 51/51 [00:01<00:00, 35.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GIF saved to ../results/gifs/task1/batch_descent.gif\n",
      "-------------------------\n",
      "\n",
      "--- Running SGD ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training with sgd: 100%|██████████| 5/5 [00:00<00:00,  5.67it/s, loss=0.1858]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final parameters: theta0=4.1563, theta1=2.8534\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating GIF frames: 100%|██████████| 501/501 [00:12<00:00, 38.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GIF saved to ../results/gifs/task1/sgd_descent.gif\n",
      "-------------------------\n",
      "\n",
      "--- Running MINI_BATCH ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training with mini_batch: 100%|██████████| 20/20 [00:00<00:00, 37.73it/s, loss=2.9748]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final parameters: theta0=4.0552, theta1=2.7542\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating GIF frames: 100%|██████████| 261/261 [00:06<00:00, 42.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GIF saved to ../results/gifs/task1/mini_batch_descent.gif\n",
      "-------------------------\n",
      "\n",
      "--- Running BATCH_MOMENTUM ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training with batch_momentum: 100%|██████████| 50/50 [00:00<00:00, 426.92it/s, loss=2.1947]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final parameters: theta0=4.0517, theta1=2.9412\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating GIF frames: 100%|██████████| 51/51 [00:01<00:00, 34.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GIF saved to ../results/gifs/task1/batch_momentum_descent.gif\n",
      "-------------------------\n",
      "\n",
      "--- Running SGD_MOMENTUM ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training with sgd_momentum: 100%|██████████| 5/5 [00:00<00:00,  6.86it/s, loss=0.0007] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final parameters: theta0=4.2920, theta1=3.3442\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating GIF frames: 100%|██████████| 501/501 [00:11<00:00, 41.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GIF saved to ../results/gifs/task1/sgd_momentum_descent.gif\n",
      "-------------------------\n",
      "\n",
      "--- Running MINI_BATCH_MOMENTUM ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training with mini_batch_momentum: 100%|██████████| 20/20 [00:00<00:00, 48.73it/s, loss=0.5091]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final parameters: theta0=4.2261, theta1=2.8844\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating GIF frames: 100%|██████████| 261/261 [00:06<00:00, 40.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GIF saved to ../results/gifs/task1/mini_batch_momentum_descent.gif\n",
      "-------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define hyperparameters for each algorithm\n",
    "algorithms_to_run = {\n",
    "    'batch': {'lr': 0.1, 'epochs': 50},\n",
    "    'sgd': {'lr': 0.01, 'epochs': 5},\n",
    "    'mini_batch': {'lr': 0.05, 'epochs': 20},\n",
    "    'batch_momentum': {'lr': 0.1, 'epochs': 50, 'momentum': 0.7},\n",
    "    'sgd_momentum': {'lr': 0.01, 'epochs': 5, 'momentum': 0.9},\n",
    "    'mini_batch_momentum': {'lr': 0.05, 'epochs': 20, 'momentum': 0.8}\n",
    "}\n",
    "\n",
    "# Run the training and generate GIFs\n",
    "for name, params in algorithms_to_run.items():\n",
    "    print(f\"--- Running {name.upper()} ---\")\n",
    "    final_t0, final_t1, history = train_linear_regression_gd(\n",
    "        x=x,\n",
    "        y=y,\n",
    "        algorithm=name,\n",
    "        learning_rate=params['lr'],\n",
    "        epochs=params['epochs'],\n",
    "        momentum=params.get('momentum', 0.9) # Default momentum if not specified\n",
    "    )\n",
    "    \n",
    "    print(f\"Final parameters: theta0={final_t0.item():.4f}, theta1={final_t1.item():.4f}\")\n",
    "    \n",
    "    # Generate and save the GIF\n",
    "    gif_path = f'../results/gifs/task1/{name}_descent.gif'\n",
    "    generate_gradient_descent_gif(history, loss_grid_for_plot, gif_path)\n",
    "    print(\"-\" * 25 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "444ae025",
   "metadata": {},
   "source": [
    "## 3. Results and Conclusion\n",
    "\n",
    "The generated GIFs are saved in the `results/gifs/task1/` directory. By observing them, we can compare the behaviors of the different algorithms.\n",
    "\n",
    "### Batch Gradient Descent\n",
    "![Batch GD](../results/gifs/task1/batch_descent.gif)\n",
    "\n",
    "*Observes a smooth, direct path to the minimum. Computationally expensive per step as it uses the entire dataset.*\n",
    "\n",
    "### Stochastic Gradient Descent (SGD)\n",
    "![SGD](../results/gifs/task1/sgd_descent.gif)\n",
    "\n",
    "*Observes a noisy, erratic path. Each step is very fast but the direction is less reliable, causing high variance in the parameter updates.*\n",
    "\n",
    "### Mini-Batch Gradient Descent\n",
    "![Mini-Batch GD](../results/gifs/task1/mini_batch_descent.gif)\n",
    "\n",
    "*A good compromise. The path is less noisy than SGD, and convergence is much faster than standard Batch GD. This is the most common approach in deep learning.*\n",
    "\n",
    "### Batch GD with Momentum\n",
    "![Batch GD Momentum](../results/gifs/task1/batch_momentum_descent.gif)\n",
    "\n",
    "*Momentum helps accelerate the descent, often overshooting the minimum slightly before settling. It's particularly effective in flatter regions of the loss landscape.*\n",
    "\n",
    "### SGD with Momentum\n",
    "![SGD Momentum](../results/gifs/task1/sgd_momentum_descent.gif)\n",
    "\n",
    "*Momentum helps to dampen the oscillations of SGD. The accumulated gradient (velocity) provides a more stable direction, leading to faster convergence.*\n",
    "\n",
    "### Mini-Batch GD with Momentum\n",
    "![Mini-Batch GD Momentum](../results/gifs/task1/mini_batch_momentum_descent.gif)\n",
    "\n",
    "*This combines the benefits of both mini-batching and momentum, often resulting in the fastest and most reliable convergence for a wide range of problems.*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "signal-synth-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
